{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e60560",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def safe_load_results(experiment_path):\n",
    "    \"\"\"\n",
    "    Load results saved with safe_save_results\n",
    "    \"\"\"\n",
    "    print(f\"Loading results from: {experiment_path}\")\n",
    "    \n",
    "    # Load metadata\n",
    "    try:\n",
    "        with open(os.path.join(experiment_path, 'metadata.json'), 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        print(f\"Experiment: {metadata['experiment_name']}\")\n",
    "        print(f\"Models: {metadata['models']}\")\n",
    "    except:\n",
    "        print(\"Warning: Could not load metadata\")\n",
    "        # Try to find model directories\n",
    "        metadata = {'models': [d for d in os.listdir(experiment_path) \n",
    "                              if os.path.isdir(os.path.join(experiment_path, d))]}\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for model_name in metadata['models']:\n",
    "        model_path = os.path.join(experiment_path, model_name)\n",
    "        print(f\"  Loading {model_name}...\")\n",
    "        \n",
    "        model_results = {}\n",
    "        \n",
    "        # Load models\n",
    "        try:\n",
    "            with open(os.path.join(model_path, 'model.pkl'), 'rb') as f:\n",
    "                model_results['model'] = pickle.load(f)\n",
    "            with open(os.path.join(model_path, 'calibrated_model.pkl'), 'rb') as f:\n",
    "                model_results['calibrated_model'] = pickle.load(f)\n",
    "            print(f\"    ‚úÖ Models loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ùå Could not load models: {e}\")\n",
    "        \n",
    "        # Load arrays\n",
    "        try:\n",
    "            model_results['predictions'] = np.load(os.path.join(model_path, 'predictions.npy'))\n",
    "            model_results['probabilities_raw'] = np.load(os.path.join(model_path, 'probabilities_raw.npy'))\n",
    "            model_results['probabilities_calibrated'] = np.load(os.path.join(model_path, 'probabilities_calibrated.npy'))\n",
    "            print(f\"    ‚úÖ Arrays loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ùå Could not load arrays: {e}\")\n",
    "        \n",
    "        # Load metrics\n",
    "        try:\n",
    "            with open(os.path.join(model_path, 'metrics.json'), 'r') as f:\n",
    "                metrics = json.load(f)\n",
    "            model_results.update(metrics)\n",
    "            print(f\"    ‚úÖ Metrics loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ùå Could not load metrics: {e}\")\n",
    "        \n",
    "        # Load calibration metrics\n",
    "        try:\n",
    "            cal_path = os.path.join(model_path, 'calibration')\n",
    "            \n",
    "            with open(os.path.join(cal_path, 'metrics.json'), 'r') as f:\n",
    "                cal_metrics = json.load(f)\n",
    "            \n",
    "            # Load calibration curves if they exist\n",
    "            try:\n",
    "                cal_metrics['calibration_curve_raw'] = {\n",
    "                    'fraction_positives': np.load(os.path.join(cal_path, 'curve_raw_frac.npy')),\n",
    "                    'mean_predicted': np.load(os.path.join(cal_path, 'curve_raw_mean.npy'))\n",
    "                }\n",
    "            except:\n",
    "                cal_metrics['calibration_curve_raw'] = None\n",
    "            \n",
    "            try:\n",
    "                cal_metrics['calibration_curve_calibrated'] = {\n",
    "                    'fraction_positives': np.load(os.path.join(cal_path, 'curve_cal_frac.npy')),\n",
    "                    'mean_predicted': np.load(os.path.join(cal_path, 'curve_cal_mean.npy'))\n",
    "                }\n",
    "            except:\n",
    "                cal_metrics['calibration_curve_calibrated'] = None\n",
    "            \n",
    "            model_results['calibration_metrics'] = cal_metrics\n",
    "            print(f\"    ‚úÖ Calibration metrics loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ùå Could not load calibration metrics: {e}\")\n",
    "        \n",
    "        # Load risk scores\n",
    "        try:\n",
    "            risk_path = os.path.join(model_path, 'risk_scores')\n",
    "            \n",
    "            risk_scores = {}\n",
    "            risk_scores['max_probability'] = np.load(os.path.join(risk_path, 'max_probability.npy'))\n",
    "            risk_scores['predicted_class'] = np.load(os.path.join(risk_path, 'predicted_class.npy'))\n",
    "            risk_scores['class_probabilities'] = np.load(os.path.join(risk_path, 'class_probabilities.npy'))\n",
    "            \n",
    "            # Load risk categories\n",
    "            risk_cats_df = pd.read_csv(os.path.join(risk_path, 'risk_categories.csv'))\n",
    "            risk_scores['risk_category'] = risk_cats_df.iloc[:, 0].values\n",
    "            \n",
    "            # Load risk performance if available\n",
    "            try:\n",
    "                with open(os.path.join(risk_path, 'risk_performance.json'), 'r') as f:\n",
    "                    risk_scores['risk_performance'] = json.load(f)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            model_results['risk_scores'] = risk_scores\n",
    "            print(f\"    ‚úÖ Risk scores loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ùå Could not load risk scores: {e}\")\n",
    "        \n",
    "        # Load feature importance\n",
    "        try:\n",
    "            feat_path = os.path.join(model_path, 'feature_importance')\n",
    "            feat_importance = {}\n",
    "            \n",
    "            if os.path.exists(feat_path):\n",
    "                # Get all files in the feature importance directory\n",
    "                feat_files = os.listdir(feat_path)\n",
    "                \n",
    "                for file_name in feat_files:\n",
    "                    file_path = os.path.join(feat_path, file_name)\n",
    "                    base_name = os.path.splitext(file_name)[0]\n",
    "                    \n",
    "                    try:\n",
    "                        if file_name.endswith('.csv'):\n",
    "                            # Try to load as DataFrame first, then as Series if it fails\n",
    "                            try:\n",
    "                                feat_importance[base_name] = pd.read_csv(file_path)\n",
    "                            except:\n",
    "                                feat_importance[base_name] = pd.read_csv(file_path, index_col=0).iloc[:, 0]\n",
    "                        \n",
    "                        elif file_name.endswith('.npy'):\n",
    "                            feat_importance[base_name] = np.load(file_path)\n",
    "                        \n",
    "                        elif file_name.endswith('.json'):\n",
    "                            with open(file_path, 'r') as f:\n",
    "                                feat_importance[base_name] = json.load(f)\n",
    "                        \n",
    "                        elif file_name.endswith('.pkl'):\n",
    "                            with open(file_path, 'rb') as f:\n",
    "                                feat_importance[base_name] = pickle.load(f)\n",
    "                    \n",
    "                    except Exception as sub_e:\n",
    "                        print(f\"      Warning: Could not load {file_name}: {sub_e}\")\n",
    "            \n",
    "            model_results['feature_importance'] = feat_importance\n",
    "            print(f\"    ‚úÖ Feature importance loaded ({len(feat_importance)} components)\")\n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ùå Could not load feature importance: {e}\")\n",
    "            model_results['feature_importance'] = {}\n",
    "        \n",
    "        results[model_name] = model_results\n",
    "    \n",
    "    print(f\"\\nüéâ Results loaded successfully!\")\n",
    "    return results\n",
    "\n",
    "\n",
    "    def predict_single_patient_risk(patient_data, trained_results, model_name='best_model'):\n",
    "    \"\"\"\n",
    "    Predict risk score for a single patient using calibrated probabilities.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    patient_data : pandas.DataFrame or numpy.array\n",
    "        Single patient's feature data (1 row)\n",
    "    trained_results : dict\n",
    "        Results from train_and_evaluate_models_with_calibration()\n",
    "    model_name : str\n",
    "        Name of the model to use for prediction\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Comprehensive risk assessment for the patient\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the calibrated model\n",
    "    if model_name == 'best_model':\n",
    "        # Automatically select best performing model based on calibrated AUC\n",
    "        best_model = max(trained_results.keys(), \n",
    "                        key=lambda x: trained_results[x].get('test_auc_calibrated', 0))\n",
    "        model_name = best_model\n",
    "    \n",
    "    calibrated_model = trained_results[model_name]['calibrated_model']\n",
    "    \n",
    "    # Get predictions\n",
    "    raw_probabilities = trained_results[model_name]['model'].predict_proba(patient_data)\n",
    "    calibrated_probabilities = calibrated_model.predict_proba(patient_data)\n",
    "    predicted_class = calibrated_model.predict(patient_data)\n",
    "    \n",
    "    # Calculate risk scores\n",
    "    max_prob = np.max(calibrated_probabilities, axis=1)[0]\n",
    "    predicted_class_idx = predicted_class[0]\n",
    "    \n",
    "    # Risk categorization\n",
    "    if max_prob >= 0.8:\n",
    "        risk_category = \"High Risk\"\n",
    "        risk_description = \"Strong evidence for PID - immediate clinical attention recommended\"\n",
    "    elif max_prob >= 0.6:\n",
    "        risk_category = \"Medium Risk\"  \n",
    "        risk_description = \"Moderate evidence for PID - further evaluation recommended\"\n",
    "    elif max_prob >= 0.4:\n",
    "        risk_category = \"Low Risk\"\n",
    "        risk_description = \"Limited evidence for PID - consider monitoring\"\n",
    "    else:\n",
    "        risk_category = \"Very Low Risk\"\n",
    "        risk_description = \"Minimal evidence for PID - routine follow-up\"\n",
    "    \n",
    "    # Create comprehensive risk report\n",
    "    risk_report = {\n",
    "        'patient_id': f\"Patient_{hash(str(patient_data.iloc[0].values)) % 10000}\",\n",
    "        'model_used': model_name,\n",
    "        \n",
    "        # Main Risk Assessment\n",
    "        'risk_score': round(max_prob * 100, 1),  # Convert to percentage\n",
    "        'risk_category': risk_category,\n",
    "        'risk_description': risk_description,\n",
    "        'predicted_class': predicted_class_idx,\n",
    "        \n",
    "        # Detailed Probabilities\n",
    "        'calibrated_probabilities': {\n",
    "            f'Class_{i}': round(prob * 100, 1) \n",
    "            for i, prob in enumerate(calibrated_probabilities[0])\n",
    "        },\n",
    "\n",
    "        'raw_probabilities': {\n",
    "            f'Class_{i}': round(prob * 100, 1) \n",
    "            for i, prob in enumerate(raw_probabilities[0])\n",
    "        },\n",
    "        \n",
    "        # Clinical Interpretation\n",
    "        'confidence_level': get_confidence_interpretation(max_prob),\n",
    "        'recommendation': get_clinical_recommendation(max_prob, predicted_class_idx),\n",
    "\n",
    "         ## Top 3 imp features based on impotance df \n",
    "        'top Features': trained_results[model_name]['feature_importance']['importance_df'].head(3).to_dict(orient='records'),\n",
    "\n",
    "        \n",
    "        \n",
    "        # Model Performance Context\n",
    "        'model_performance': {\n",
    "            # 'model_accuracy': round(trained_results[model_name]['test_accuracy'] * 100, 1),\n",
    "            # 'model_f1_score': round(trained_results[model_name]['test_f1'] * 100, 1),\n",
    "            # 'calibration_quality': get_calibration_quality(trained_results[model_name]),\n",
    "            # 'accuracy_out_of_10': get_accuracy_out_of_10(trained_results[model_name]['test_accuracy']),\n",
    "            # 'precision_out_of_10': get_accuracy_out_of_10(trained_results[model_name]['test_precision']),\n",
    "            # 'recall_out_of_10': get_accuracy_out_of_10(trained_results[model_name]['test_recall']),\n",
    "            'balanced_accuracy': get_accuracy_out_of_10(trained_results[model_name]['test_balanced_accuracy']),\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return risk_report\n",
    "\n",
    "\n",
    "def get_accuracy_out_of_10(metric_value):\n",
    "    \"\"\"Convert decimal accuracy/precision/recall to 'out of 10' format.\"\"\"\n",
    "    out_of_10 = round(metric_value * 10)\n",
    "    return f\"{out_of_10} out of 10 Patients are predicted correctly\"\n",
    "\n",
    "\n",
    "def get_detailed_performance_interpretation(metric_value, metric_name):\n",
    "    \"\"\"Provide detailed interpretation of model performance metrics.\"\"\"\n",
    "    out_of_10 = round(metric_value * 10)\n",
    "    percentage = round(metric_value * 100, 1)\n",
    "    \n",
    "    interpretations = {\n",
    "        'accuracy': f\"Correctly predicts {out_of_10} out of 10 cases overall\",\n",
    "        'precision': f\"When predicting PID, {out_of_10} out of 10 predictions are correct\",\n",
    "        'recall': f\"Identifies {out_of_10} out of 10 actual PID cases\",\n",
    "        'f1': f\"Balanced performance score: {percentage}%\"\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'out_of_10': f\"{out_of_10} out of 10\",\n",
    "        'percentage': f\"{percentage}%\",\n",
    "        'interpretation': interpretations.get(metric_name, f\"{out_of_10} out of 10\")\n",
    "    }\n",
    "def get_confidence_interpretation(probability):\n",
    "    \n",
    "    \"\"\"Convert probability to confidence interpretation.\"\"\"\n",
    "    if probability >= 0.9:\n",
    "        return \"Very High Confidence\"\n",
    "    elif probability >= 0.8:\n",
    "        return \"High Confidence\"\n",
    "    elif probability >= 0.7:\n",
    "        return \"Moderate Confidence\"\n",
    "    elif probability >= 0.6:\n",
    "        return \"Fair Confidence\"\n",
    "    else:\n",
    "        return \"Low Confidence\"\n",
    "\n",
    "\n",
    "def get_clinical_recommendation(probability, predicted_class):\n",
    "    \"\"\"Provide clinical recommendations based on risk score.\"\"\"\n",
    "    if probability >= 0.8:\n",
    "        return \"Immediate clinical evaluation recommended. Consider antibiotic treatment.\"\n",
    "    elif probability >= 0.6:\n",
    "        return \"Further diagnostic testing recommended (e.g., pelvic exam, lab tests).\"\n",
    "    elif probability >= 0.4:\n",
    "        return \"Monitor symptoms closely. Consider follow-up if symptoms persist.\"\n",
    "    else:\n",
    "        return \"Routine follow-up. Educate patient on when to seek care.\"\n",
    "\n",
    "\n",
    "def get_calibration_quality(model_results):\n",
    "    \"\"\"Assess calibration quality of the model.\"\"\"\n",
    "    brier_improvement = model_results['calibration_metrics'].get('brier_improvement', 0)\n",
    "    if brier_improvement > 0.05:\n",
    "        return \"Excellent calibration improvement\"\n",
    "    elif brier_improvement > 0.02:\n",
    "        return \"Good calibration improvement\"\n",
    "    elif brier_improvement > 0.01:\n",
    "        return \"Fair calibration improvement\"\n",
    "    else:\n",
    "        return \"Minimal calibration improvement\"\n",
    "\n",
    "\n",
    "def print_patient_risk_report(risk_report):\n",
    "    \"\"\"Print a formatted risk report for a patient.\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(f\"PID RISK ASSESSMENT REPORT\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Patient ID: {risk_report['patient_id']}\")\n",
    "    print(f\"Model Used: {risk_report['model_used']}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"RISK ASSESSMENT:\")\n",
    "    print(f\"  Risk Score: {risk_report['risk_score']}%\")\n",
    "    print(f\"  Risk Category: {risk_report['risk_category']}\")\n",
    "    print(f\"  Confidence: {risk_report['confidence_level']}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"CLINICAL INTERPRETATION:\")\n",
    "    print(f\"  {risk_report['risk_description']}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"RECOMMENDATION:\")\n",
    "    print(f\"  {risk_report['recommendation']}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"DETAILED PROBABILITIES:\")\n",
    "    print(\"  Calibrated Probabilities:\")\n",
    "    for class_name, prob in risk_report['calibrated_probabilities'].items():\n",
    "        print(f\"    {class_name}: {prob}%\")\n",
    "    print()\n",
    "\n",
    "# Top 3 Features\n",
    "    print(\"\\nüîç Top 3 Important Features:\")\n",
    "    print(f\"{'Rank':<5} {'Feature':<20} {'Importance':<10}\")\n",
    "    print(\"-\" * 40)\n",
    "    for idx, feature in enumerate(risk_report['top Features'], start=1):\n",
    "        print(f\"{idx:<5} {feature['feature']:<20} {feature['importance']:<10.4f}\")\n",
    "    \n",
    "    print(\"MODEL PERFORMANCE CONTEXT:\")\n",
    "    perf = risk_report['model_performance']\n",
    "    print(f\"balanced_accuracy: {perf['balanced_accuracy']}\")\n",
    "    # print(f\"  Model Accuracy: {perf['model_accuracy']}%\")\n",
    "    # print(f\"  Model F1-Score: {perf['model_f1_score']}%\")\n",
    "    # print(f\"  Calibration Quality: {perf['calibration_quality']}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "\n",
    "def batch_predict_patients(patients_data, trained_results, model_name='best_model'):\n",
    "    \"\"\"\n",
    "    Predict risk scores for multiple patients.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    patients_data : pandas.DataFrame\n",
    "        Multiple patients' feature data\n",
    "    trained_results : dict\n",
    "        Results from train_and_evaluate_models_with_calibration()\n",
    "    model_name : str\n",
    "        Name of the model to use for prediction\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list : List of risk reports for each patient\n",
    "    \"\"\"\n",
    "    risk_reports = []\n",
    "    \n",
    "    for idx in range(len(patients_data)):\n",
    "        patient_data = patients_data.iloc[idx:idx+1]  # Keep as DataFrame\n",
    "        risk_report = predict_single_patient_risk(patient_data, trained_results, model_name)\n",
    "        risk_report['patient_index'] = idx\n",
    "        risk_reports.append(risk_report)\n",
    "    \n",
    "    return risk_reports\n",
    "\n",
    "\n",
    "def create_risk_summary_dashboard(risk_reports):\n",
    "    \"\"\"Create a summary dashboard of multiple patient risk assessments.\"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Extract key metrics\n",
    "    summary_data = []\n",
    "    for report in risk_reports:\n",
    "        summary_data.append({\n",
    "            'Patient_ID': report['patient_id'],\n",
    "            'Risk_Score': report['risk_score'],\n",
    "            'Risk_Category': report['risk_category'],\n",
    "            'Confidence': report['confidence_level'],\n",
    "            'Predicted_Class': report['predicted_class']\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Risk distribution\n",
    "    risk_distribution = summary_df['Risk_Category'].value_counts()\n",
    "    \n",
    "    print(\"PATIENT RISK SUMMARY DASHBOARD\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total Patients Assessed: {len(risk_reports)}\")\n",
    "    print()\n",
    "    print(\"Risk Distribution:\")\n",
    "    for category, count in risk_distribution.items():\n",
    "        percentage = (count / len(risk_reports)) * 100\n",
    "        print(f\"  {category}: {count} patients ({percentage:.1f}%)\")\n",
    "    print()\n",
    "    \n",
    "    print(\"High Risk Patients (‚â•80% risk score):\")\n",
    "    high_risk = summary_df[summary_df['Risk_Score'] >= 80]\n",
    "    if len(high_risk) > 0:\n",
    "        for _, patient in high_risk.iterrows():\n",
    "            print(f\"  {patient['Patient_ID']}: {patient['Risk_Score']}% risk\")\n",
    "    else:\n",
    "        print(\"  No high-risk patients identified\")\n",
    "    \n",
    "    return summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3c4c3aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_path=path=r\"C:\\Users\\ashish.kumar1\\CSL Classification\\my_ml_projects\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eb53e377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading results from: C:\\Users\\ashish.kumar1\\CSL Classification\\my_ml_projects\n",
      "Experiment: C:\\Users\\ashish.kumar1\\CSL Classification\\my_ml_projects\n",
      "Models: ['RandomForest', 'XGBoost', 'LightGBM', 'LogisticRegression', 'GradientBoosting']\n",
      "  Loading RandomForest...\n",
      "    ‚úÖ Models loaded\n",
      "    ‚úÖ Arrays loaded\n",
      "    ‚úÖ Metrics loaded\n",
      "    ‚úÖ Calibration metrics loaded\n",
      "    ‚úÖ Risk scores loaded\n",
      "    ‚úÖ Feature importance loaded (5 components)\n",
      "  Loading XGBoost...\n",
      "    ‚úÖ Models loaded\n",
      "    ‚úÖ Arrays loaded\n",
      "    ‚úÖ Metrics loaded\n",
      "    ‚úÖ Calibration metrics loaded\n",
      "    ‚úÖ Risk scores loaded\n",
      "    ‚úÖ Feature importance loaded (5 components)\n",
      "  Loading LightGBM...\n",
      "    ‚úÖ Models loaded\n",
      "    ‚úÖ Arrays loaded\n",
      "    ‚úÖ Metrics loaded\n",
      "    ‚úÖ Calibration metrics loaded\n",
      "    ‚úÖ Risk scores loaded\n",
      "    ‚úÖ Feature importance loaded (5 components)\n",
      "  Loading LogisticRegression...\n",
      "    ‚úÖ Models loaded\n",
      "    ‚úÖ Arrays loaded\n",
      "    ‚úÖ Metrics loaded\n",
      "    ‚úÖ Calibration metrics loaded\n",
      "    ‚úÖ Risk scores loaded\n",
      "    ‚úÖ Feature importance loaded (5 components)\n",
      "  Loading GradientBoosting...\n",
      "    ‚úÖ Models loaded\n",
      "    ‚úÖ Arrays loaded\n",
      "    ‚úÖ Metrics loaded\n",
      "    ‚úÖ Calibration metrics loaded\n",
      "    ‚úÖ Risk scores loaded\n",
      "    ‚úÖ Feature importance loaded (3 components)\n",
      "\n",
      "üéâ Results loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load them late\n",
    "loaded_results = safe_load_results(custom_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0acde774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_patient_risk(patient_data, trained_results, model_name='best_model'):\n",
    "    \"\"\"\n",
    "    Predict risk score for a single patient using calibrated probabilities.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    patient_data : pandas.DataFrame or numpy.array\n",
    "        Single patient's feature data (1 row)\n",
    "    trained_results : dict\n",
    "        Results from train_and_evaluate_models_with_calibration()\n",
    "    model_name : str\n",
    "        Name of the model to use for prediction\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Comprehensive risk assessment for the patient\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the calibrated model\n",
    "    if model_name == 'best_model':\n",
    "        # Automatically select best performing model based on calibrated AUC\n",
    "        best_model = max(trained_results.keys(), \n",
    "                        key=lambda x: trained_results[x].get('test_auc_calibrated', 0))\n",
    "        model_name = best_model\n",
    "    \n",
    "    calibrated_model = trained_results[model_name]['calibrated_model']\n",
    "    \n",
    "    # Get predictions\n",
    "    raw_probabilities = trained_results[model_name]['model'].predict_proba(patient_data)\n",
    "    calibrated_probabilities = calibrated_model.predict_proba(patient_data)\n",
    "    predicted_class = calibrated_model.predict(patient_data)\n",
    "    \n",
    "    # Calculate risk scores\n",
    "    max_prob = np.max(calibrated_probabilities, axis=1)[0]\n",
    "    predicted_class_idx = predicted_class[0]\n",
    "    \n",
    "    # Risk categorization\n",
    "    if max_prob >= 0.8:\n",
    "        risk_category = \"High Risk\"\n",
    "        risk_description = \"Strong evidence for PID - immediate clinical attention recommended\"\n",
    "    elif max_prob >= 0.6:\n",
    "        risk_category = \"Medium Risk\"  \n",
    "        risk_description = \"Moderate evidence for PID - further evaluation recommended\"\n",
    "    elif max_prob >= 0.4:\n",
    "        risk_category = \"Low Risk\"\n",
    "        risk_description = \"Limited evidence for PID - consider monitoring\"\n",
    "    else:\n",
    "        risk_category = \"Very Low Risk\"\n",
    "        risk_description = \"Minimal evidence for PID - routine follow-up\"\n",
    "    \n",
    "    # Create comprehensive risk report\n",
    "    risk_report = {\n",
    "        'patient_id': f\"Patient_{hash(str(patient_data.iloc[0].values)) % 10000}\",\n",
    "        'model_used': model_name,\n",
    "        \n",
    "        # Main Risk Assessment\n",
    "        'risk_score': round(max_prob * 100, 1),  # Convert to percentage\n",
    "        'risk_category': risk_category,\n",
    "        'risk_description': risk_description,\n",
    "        'predicted_class': predicted_class_idx,\n",
    "        \n",
    "        # Detailed Probabilities\n",
    "        'calibrated_probabilities': {\n",
    "            f'Class_{i}': round(prob * 100, 1) \n",
    "            for i, prob in enumerate(calibrated_probabilities[0])\n",
    "        },\n",
    "\n",
    "        'raw_probabilities': {\n",
    "            f'Class_{i}': round(prob * 100, 1) \n",
    "            for i, prob in enumerate(raw_probabilities[0])\n",
    "        },\n",
    "        \n",
    "        # Clinical Interpretation\n",
    "        'confidence_level': get_confidence_interpretation(max_prob),\n",
    "        'recommendation': get_clinical_recommendation(max_prob, predicted_class_idx),\n",
    "\n",
    "         ## Top 3 imp features based on impotance df \n",
    "        'top Features': trained_results[model_name]['feature_importance']['importance_df'].head(3).to_dict(orient='records'),\n",
    "\n",
    "        \n",
    "        \n",
    "        # Model Performance Context\n",
    "        'model_performance': {\n",
    "            # 'model_accuracy': round(trained_results[model_name]['test_accuracy'] * 100, 1),\n",
    "            # 'model_f1_score': round(trained_results[model_name]['test_f1'] * 100, 1),\n",
    "            # 'calibration_quality': get_calibration_quality(trained_results[model_name]),\n",
    "            # 'accuracy_out_of_10': get_accuracy_out_of_10(trained_results[model_name]['test_accuracy']),\n",
    "            # 'precision_out_of_10': get_accuracy_out_of_10(trained_results[model_name]['test_precision']),\n",
    "            # 'recall_out_of_10': get_accuracy_out_of_10(trained_results[model_name]['test_recall']),\n",
    "            'balanced_accuracy': get_accuracy_out_of_10(trained_results[model_name]['test_balanced_accuracy']),\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return risk_report\n",
    "\n",
    "\n",
    "def get_accuracy_out_of_10(metric_value):\n",
    "    \"\"\"Convert decimal accuracy/precision/recall to 'out of 10' format.\"\"\"\n",
    "    out_of_10 = round(metric_value * 10)\n",
    "    return f\"{out_of_10} out of 10 Patients are predicted correctly\"\n",
    "\n",
    "\n",
    "def get_detailed_performance_interpretation(metric_value, metric_name):\n",
    "    \"\"\"Provide detailed interpretation of model performance metrics.\"\"\"\n",
    "    out_of_10 = round(metric_value * 10)\n",
    "    percentage = round(metric_value * 100, 1)\n",
    "    \n",
    "    interpretations = {\n",
    "        'accuracy': f\"Correctly predicts {out_of_10} out of 10 cases overall\",\n",
    "        'precision': f\"When predicting PID, {out_of_10} out of 10 predictions are correct\",\n",
    "        'recall': f\"Identifies {out_of_10} out of 10 actual PID cases\",\n",
    "        'f1': f\"Balanced performance score: {percentage}%\"\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'out_of_10': f\"{out_of_10} out of 10\",\n",
    "        'percentage': f\"{percentage}%\",\n",
    "        'interpretation': interpretations.get(metric_name, f\"{out_of_10} out of 10\")\n",
    "    }\n",
    "def get_confidence_interpretation(probability):\n",
    "    \n",
    "    \"\"\"Convert probability to confidence interpretation.\"\"\"\n",
    "    if probability >= 0.9:\n",
    "        return \"Very High Confidence\"\n",
    "    elif probability >= 0.8:\n",
    "        return \"High Confidence\"\n",
    "    elif probability >= 0.7:\n",
    "        return \"Moderate Confidence\"\n",
    "    elif probability >= 0.6:\n",
    "        return \"Fair Confidence\"\n",
    "    else:\n",
    "        return \"Low Confidence\"\n",
    "\n",
    "\n",
    "def get_clinical_recommendation(probability, predicted_class):\n",
    "    \"\"\"Provide clinical recommendations based on risk score.\"\"\"\n",
    "    if probability >= 0.8:\n",
    "        return \"Immediate clinical evaluation recommended. Consider antibiotic treatment.\"\n",
    "    elif probability >= 0.6:\n",
    "        return \"Further diagnostic testing recommended (e.g., pelvic exam, lab tests).\"\n",
    "    elif probability >= 0.4:\n",
    "        return \"Monitor symptoms closely. Consider follow-up if symptoms persist.\"\n",
    "    else:\n",
    "        return \"Routine follow-up. Educate patient on when to seek care.\"\n",
    "\n",
    "\n",
    "def get_calibration_quality(model_results):\n",
    "    \"\"\"Assess calibration quality of the model.\"\"\"\n",
    "    brier_improvement = model_results['calibration_metrics'].get('brier_improvement', 0)\n",
    "    if brier_improvement > 0.05:\n",
    "        return \"Excellent calibration improvement\"\n",
    "    elif brier_improvement > 0.02:\n",
    "        return \"Good calibration improvement\"\n",
    "    elif brier_improvement > 0.01:\n",
    "        return \"Fair calibration improvement\"\n",
    "    else:\n",
    "        return \"Minimal calibration improvement\"\n",
    "\n",
    "\n",
    "def print_patient_risk_report(risk_report):\n",
    "    \"\"\"Print a formatted risk report for a patient.\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(f\"PID RISK ASSESSMENT REPORT\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Patient ID: {risk_report['patient_id']}\")\n",
    "    print(f\"Model Used: {risk_report['model_used']}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"RISK ASSESSMENT:\")\n",
    "    print(f\"  Risk Score: {risk_report['risk_score']}%\")\n",
    "    print(f\"  Risk Category: {risk_report['risk_category']}\")\n",
    "    print(f\"  Confidence: {risk_report['confidence_level']}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"CLINICAL INTERPRETATION:\")\n",
    "    print(f\"  {risk_report['risk_description']}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"RECOMMENDATION:\")\n",
    "    print(f\"  {risk_report['recommendation']}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"DETAILED PROBABILITIES:\")\n",
    "    print(\"  Calibrated Probabilities:\")\n",
    "    for class_name, prob in risk_report['calibrated_probabilities'].items():\n",
    "        print(f\"    {class_name}: {prob}%\")\n",
    "    print()\n",
    "\n",
    "# Top 3 Features\n",
    "    print(\"\\nüîç Top 3 Important Features:\")\n",
    "    print(f\"{'Rank':<5} {'Feature':<20} {'Importance':<10}\")\n",
    "    print(\"-\" * 40)\n",
    "    for idx, feature in enumerate(risk_report['top Features'], start=1):\n",
    "        print(f\"{idx:<5} {feature['feature']:<20} {feature['importance']:<10.4f}\")\n",
    "    \n",
    "    print(\"MODEL PERFORMANCE CONTEXT:\")\n",
    "    perf = risk_report['model_performance']\n",
    "    print(f\"balanced_accuracy: {perf['balanced_accuracy']}\")\n",
    "    # print(f\"  Model Accuracy: {perf['model_accuracy']}%\")\n",
    "    # print(f\"  Model F1-Score: {perf['model_f1_score']}%\")\n",
    "    # print(f\"  Calibration Quality: {perf['calibration_quality']}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "\n",
    "def batch_predict_patients(patients_data, trained_results, model_name='best_model'):\n",
    "    \"\"\"\n",
    "    Predict risk scores for multiple patients.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    patients_data : pandas.DataFrame\n",
    "        Multiple patients' feature data\n",
    "    trained_results : dict\n",
    "        Results from train_and_evaluate_models_with_calibration()\n",
    "    model_name : str\n",
    "        Name of the model to use for prediction\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list : List of risk reports for each patient\n",
    "    \"\"\"\n",
    "    risk_reports = []\n",
    "    \n",
    "    for idx in range(len(patients_data)):\n",
    "        patient_data = patients_data.iloc[idx:idx+1]  # Keep as DataFrame\n",
    "        risk_report = predict_single_patient_risk(patient_data, trained_results, model_name)\n",
    "        risk_report['patient_index'] = idx\n",
    "        risk_reports.append(risk_report)\n",
    "    \n",
    "    return risk_reports\n",
    "\n",
    "\n",
    "def create_risk_summary_dashboard(risk_reports):\n",
    "    \"\"\"Create a summary dashboard of multiple patient risk assessments.\"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Extract key metrics\n",
    "    summary_data = []\n",
    "    for report in risk_reports:\n",
    "        summary_data.append({\n",
    "            'Patient_ID': report['patient_id'],\n",
    "            'Risk_Score': report['risk_score'],\n",
    "            'Risk_Category': report['risk_category'],\n",
    "            'Confidence': report['confidence_level'],\n",
    "            'Predicted_Class': report['predicted_class']\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Risk distribution\n",
    "    risk_distribution = summary_df['Risk_Category'].value_counts()\n",
    "    \n",
    "    print(\"PATIENT RISK SUMMARY DASHBOARD\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total Patients Assessed: {len(risk_reports)}\")\n",
    "    print()\n",
    "    print(\"Risk Distribution:\")\n",
    "    for category, count in risk_distribution.items():\n",
    "        percentage = (count / len(risk_reports)) * 100\n",
    "        print(f\"  {category}: {count} patients ({percentage:.1f}%)\")\n",
    "    print()\n",
    "    \n",
    "    print(\"High Risk Patients (‚â•80% risk score):\")\n",
    "    high_risk = summary_df[summary_df['Risk_Score'] >= 80]\n",
    "    if len(high_risk) > 0:\n",
    "        for _, patient in high_risk.iterrows():\n",
    "            print(f\"  {patient['Patient_ID']}: {patient['Risk_Score']}% risk\")\n",
    "    else:\n",
    "        print(\"  No high-risk patients identified\")\n",
    "    \n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dcf72464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LogisticRegression'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the best model out of the all model based on balace accuracy \n",
    "best_model_name = max(loaded_results.keys(),\n",
    "                       key=lambda x: loaded_results[x]['test_balanced_accuracy'])\n",
    "best_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1c4198cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ashish.kumar1\\CSL Classification\\classification\\Lib\\site-packages\\sklearn\\utils\\validation.py:2732: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\ashish.kumar1\\CSL Classification\\classification\\Lib\\site-packages\\sklearn\\utils\\validation.py:2732: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\ashish.kumar1\\CSL Classification\\classification\\Lib\\site-packages\\sklearn\\utils\\validation.py:2732: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\ashish.kumar1\\CSL Classification\\classification\\Lib\\site-packages\\sklearn\\utils\\validation.py:2732: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\ashish.kumar1\\CSL Classification\\classification\\Lib\\site-packages\\sklearn\\utils\\validation.py:2732: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\ashish.kumar1\\CSL Classification\\classification\\Lib\\site-packages\\sklearn\\utils\\validation.py:2732: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\ashish.kumar1\\CSL Classification\\classification\\Lib\\site-packages\\sklearn\\utils\\validation.py:2732: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### Feed the patient data to the model\n",
    "# Load a single patient's data\n",
    "patient_data=pd.read_csv(r\"C:\\Users\\ashish.kumar1\\CSL Classification\\single_patient_data.csv\")\n",
    "\n",
    "# Predict risk for the single patient\n",
    "final_result=predict_single_patient_risk(patient_data, loaded_results, model_name=best_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ff3009b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient_id: Patient_837\n",
      "risk_score: 97.2\n",
      "risk_category: High Risk\n",
      "predicted_class: 0\n",
      "confidence_level: Very High Confidence\n",
      "recommendation: Immediate clinical evaluation recommended. Consider antibiotic treatment.\n",
      "top Features: [{'feature': 'PX_STEM_CELL_TRANSPLANT', 'importance': 2.7347331804581523}, {'feature': 'MED_CYTOTOXIC_CHEMOTHERAPY', 'importance': 2.680039046828346}, {'feature': 'CD_HEMATOLOGIC_MALIGNANCIES', 'importance': 2.642911822075066}]\n",
      "model_performance: {'balanced_accuracy': '8 out of 10 Patients are predicted correctly'}\n"
     ]
    }
   ],
   "source": [
    "## get the id,risk score, risk category, confidence level, predicted class, top 3 features,recommendation, model performance \n",
    "for keys in final_result.keys():\n",
    "    if keys in ['patient_id', 'risk_score', 'risk_category', 'confidence_level', \n",
    "                          'predicted_class', 'top Features', 'recommendation', \n",
    "                          'model_performance']:\n",
    "        print(f\"{keys}: {final_result[keys]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794d6f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b8403c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
